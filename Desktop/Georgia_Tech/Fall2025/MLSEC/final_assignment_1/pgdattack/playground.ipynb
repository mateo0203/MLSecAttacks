{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualizing Adversarial Attacks on Image Classification Models**\n",
    "\n",
    "This notebook helps visualize the adversarial attack that you performed as part of your assignment by comparing original images and their adversarially perturbed versions.  \n",
    "For each image, we show:\n",
    "- The **original image** and its correct label.\n",
    "- The **adversarial image** generated by an attack.\n",
    "- The **new label predicted by the model after the attack**.\n",
    "\n",
    "This helps understand how adversarial attacks can deceive deep learning models and change their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Inverse normalization based on ResNet50_Weights.DEFAULT normalization parameters\n",
    "def inv_normalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
    "    inv_tensor = tensor * std + mean\n",
    "    return torch.clamp(inv_tensor, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_comparison(original, adversarial, title_original=\"Original\", title_adv=\"Adversarial\"):\n",
    "    # Move tensors to CPU, convert to NumPy arrays and transpose to HWC format\n",
    "    orig_np = original.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    adv_np = adversarial.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(orig_np)\n",
    "    axs[0].set_title(title_original)\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(adv_np)\n",
    "    axs[1].set_title(title_adv)\n",
    "    axs[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageNet labels from the JSON file\n",
    "with open(\"../data/imagenet_labels.json\", \"r\") as f:\n",
    "    imagenet_classes = json.load(f)\n",
    "\n",
    "# Function to get class label\n",
    "def get_label(class_index):\n",
    "    return imagenet_classes[class_index] if 0 <= class_index < len(imagenet_classes) else f\"Class {class_index}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load and Visualize Adversarial Images**\n",
    "This step loads the saved results and visualizes:\n",
    "- The original image with its true label.\n",
    "- The adversarial image with the label **misclassified** by the model.\n",
    "\n",
    "### **Expected Outputs**\n",
    "Each pair of images will have:\n",
    "1. **Original Image** with its correct label.\n",
    "2. **Adversarial Image** with the label predicted after the attack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from the attack\n",
    "results_path = \"path/to/results\"\n",
    "num_images = 5\n",
    "results = torch.load(results_path)\n",
    "\n",
    "adv_images = results[\"adv_images\"] \n",
    "original_images = results[\"clean_images\"] \n",
    "labels = results[\"labels\"] \n",
    "adv_labels_predictions = results[\"adv_labels_predictions\"]\n",
    "\n",
    "num_show = min(num_images, adv_images.shape[0])\n",
    "\n",
    "# de-normalize both sets for proper visualization.\n",
    "original_images = torch.stack([inv_normalize(img) for img in original_images])\n",
    "adv_images_to_show = torch.stack([inv_normalize(img) for img in adv_images[:num_show]])\n",
    "\n",
    "# Loop over the pairs and display them side by side.\n",
    "for i in range(num_show):\n",
    "        original_label = imagenet_classes[labels[i].item()]\n",
    "        adv_label = imagenet_classes[adv_labels_predictions[i]]\n",
    "\n",
    "        show_comparison(original_images[i], adv_images_to_show[i],\n",
    "                        title_original=f\"Original: {original_label}\",\n",
    "                        title_adv=f\"Adversarial: {adv_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion** \n",
    "By comparing the original and adversarial images, we can see how small perturbations can trick deep learning models into making incorrect predictions.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Adversarial examples** are visually similar to original images but mislead the model.\n",
    "- **Even state-of-the-art models like ResNet50 can be fooled** by adversarial perturbations.\n",
    "\n",
    "Feel free to experiment with different adversarial attack methods and analyze their effects!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls_ass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
